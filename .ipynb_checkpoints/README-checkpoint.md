# Лабораторная работа: Моделирование классификации (Перцептрон и Метод опорных векторов)

## Техническое задание (ТЗ)

**Цель:**  
Реализовать с нуля (без использования `sklearn` для алгоритмов) базовые модели классификации:
- Линейный **Перцептрон** (логистическая регрессия);
- Линейный **SVM (метод опорных векторов)**.

Провести обучение и оценку моделей на датасете **Breast Cancer Wisconsin (Diagnostic)**.  
Рассчитать метрики качества, визуализировать процесс обучения и сделать выводы.

---

## Используемые технологии

- **Язык:** Python 3.12  
- **Библиотеки:** `numpy`, `matplotlib`, `seaborn`, `sklearn.datasets`, `sklearn.model_selection`, `sklearn.preprocessing`  
  *(из sklearn используются только данные и масштабирование — алгоритмы реализованы вручную)*

---

## Описание данных

**Датасет:** `load_breast_cancer()`  
- 569 образцов (пациентов)  
- 30 числовых признаков (например: радиус, текстура, периметр, гладкость и т.д.)  
- Классы:  
  - `0` — злокачественная опухоль (malignant)  
  - `1` — доброкачественная опухоль (benign)

---

## Подготовка данных

1. 
2. 

## Материалы 
![SVM](1.png)

---
1. где:
  - первый член ½‖w‖² = регуляризация (контролирует “жёсткость” границы),
  - второй член C * hinge = штраф за ошибочно классифицированные или близкие точки.
2. C определяет баланс между жёсткостью и точностью:
  - большой C → сильнее штрафуем ошибки → граница подстраивается под данные (рискуем переобучиться);
  - маленький C → мягче, больше допускает ошибок, но модель устойчивее.
3. В мат виде

    ![Итоговая функция потерь](Итоговая%20функция%20потерь.png)
    - коэффицент max(n - 
---
1. Градиент функции потерь SVM
   - mask * y_pm → вектор, где участвуют только ошибочные точки (у правильных margin≤0, mask=0);
   - X.T @ (...) → матричное умножение: суммирование вкладов по всем признакам;
   - делим на m → усредняем;
   - умножаем на -C → учитываем штраф.
2. В мат виде

    ![градиент функции потерь SVM](градиент%20функции%20потерь%20SVM.png)

